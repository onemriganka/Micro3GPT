{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time  # Importing time module for tracking\n",
        "\n",
        "# Define the GPT model\n",
        "class micro3GPT(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size, num_layers):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.transformer = nn.Transformer(hidden_size, num_layers)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src = self.embedding(src)\n",
        "        tgt = self.embedding(tgt)\n",
        "        out = self.transformer(src, tgt)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "Y6E7wcTYbFpL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the dataset\n",
        "data = [\"The quick brown fox jumps over the lazy dog.\",\n",
        "        \"The quick brown fox jumps over the lazy dog again.\",\n",
        "        \"The quick brown fox jumps over the lazy dog one more time.\",\n",
        "        \"The quick brown fox jumps over the lazy dog once more time and .\",]"
      ],
      "metadata": {
        "id": "KNobocywbDUy"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4oVFk-aTIEz5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "820dc07d-d9bc-4ec9-ad9e-362c380c86fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed in 3.73 seconds\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Convert text to numerical data\n",
        "word_to_index = {}\n",
        "index_to_word = {}\n",
        "for sentence in data:\n",
        "    for word in sentence.split():\n",
        "        if word not in word_to_index:\n",
        "            index = len(word_to_index)\n",
        "            word_to_index[word] = index\n",
        "            index_to_word[index] = word\n",
        "\n",
        "X = torch.tensor([word_to_index[word] for sentence in data for word in sentence.split()[:-1]])\n",
        "Y = torch.tensor([word_to_index[word] for sentence in data for word in sentence.split()[1:]])\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move tensors to the device\n",
        "X = X.to(device)\n",
        "Y = Y.to(device)\n",
        "\n",
        "# Define model parameters\n",
        "vocab_size = len(word_to_index)\n",
        "embedding_size = 128\n",
        "hidden_size = 128\n",
        "num_layers = 1\n",
        "\n",
        "# Define the model, loss function, and optimizer\n",
        "model = micro3GPT(vocab_size, embedding_size, hidden_size, num_layers).to(device)  # Move model to device\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model and measure the time\n",
        "start_time = time.time()  # Start timer\n",
        "for epoch in range(100):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X, Y)\n",
        "    loss = criterion(outputs.view(-1, vocab_size), Y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    #print(f\"Epoch {epoch}: loss={loss.item()}\")\n",
        "end_time = time.time()  # End timer\n",
        "training_time = end_time - start_time\n",
        "print(f\"Training completed in {training_time:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text given a prompt and measure the time\n",
        "prompt = \"The quick\"\n",
        "start_time = time.time()  # Start timer for text generation\n",
        "prompt_tensor = torch.tensor([word_to_index[word] for word in prompt.split()]).to(device)  # Move input to device\n",
        "output = model(prompt_tensor.unsqueeze(0), prompt_tensor.unsqueeze(0))\n",
        "next_word_index = torch.argmax(output[-1]).item()\n",
        "next_word = index_to_word.get(next_word_index, \"<unk>\")\n",
        "end_time = time.time()  # End timer for text generation\n",
        "generation_time = end_time - start_time\n",
        "print(f\"Generated text: {prompt + ' ' + next_word}\")\n",
        "print(f\"Text generation completed in {generation_time:.4f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QOJhWlzahb5",
        "outputId": "4f8377c2-64a1-4f18-fd39-fcaed4e4f0cd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text: The quick <unk>\n",
            "Text generation completed in 0.0149 seconds\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}